import asyncio
import json
import logging
import re
from typing import Dict, Any, List
from groq import Groq, APIError
from config import GROQ_API_KEY, GROQ_MODEL

# Configure logging
logger = logging.getLogger(__name__)

class AIInterface:
    """Interface for interacting with the Groq Language Model for security tasks."""

    def __init__(self):
        if not GROQ_API_KEY:
            raise ValueError("GROQ_API_KEY environment variable not set.")
        self.client = Groq(api_key=GROQ_API_KEY)
        self.model = GROQ_MODEL
        # Patterns for destructive commands to be sanitized from generated payloads
        self.dangerous_patterns = [
            r'rm\s+-rf',          # File deletion
            r'format\s+c:',       # Disk formatting
            r'del\s+/[qsf]',      # Windows delete
            r'DROP\s+DATABASE',   # Database destruction
            r'TRUNCATE\s+TABLE',  # Table destruction
            r'SHUTDOWN',          # SQL shutdown
            r'system\(',          # System command execution in some languages
            r'exec\(',            # Code execution
        ]

    def _sanitize_and_filter_payloads(self, payloads: List[str]) -> List[str]:
        """Filters out payloads that contain dangerous or destructive patterns."""
        sanitized_payloads = []
        for payload in payloads:
            is_safe = True
            for pattern in self.dangerous_patterns:
                if re.search(pattern, payload, re.IGNORECASE):
                    logger.warning(f"Sanitizing unsafe payload generated by LLM: {payload}")
                    is_safe = False
                    break
            if is_safe:
                sanitized_payloads.append(payload)
        return sanitized_payloads

    async def _call_llm(self, messages: List[Dict[str, str]], max_tokens: int = 500, is_json: bool = False) -> str:
        """Generic method to call the Groq Chat Completions API."""
        try:
            response_format = {"type": "json_object"} if is_json else None
            def sync_call():
                return self.client.chat.completions.create(
                    messages=messages,
                    model=self.model,
                    max_tokens=max_tokens,
                    temperature=0.4,
                    response_format=response_format
                )
            chat_completion = await asyncio.to_thread(sync_call)
            return chat_completion.choices[0].message.content
        except APIError as e:
            logger.error(f"Groq API Error: {e}")
            raise

    async def generate_payloads(self, vuln_type: str, context: Dict[str, Any], count: int = 10) -> List[str]:
        """Generate, sanitize, and return context-aware payloads using the Groq LLM."""
        system_prompt = f"""
        You are a security expert. Your task is to generate {count} diverse and effective payloads for a '{vuln_type}' vulnerability test.
        IMPORTANT: The payloads must be non-destructive and safe for testing. DO NOT generate payloads that could delete data, drop tables, shut down services, or execute arbitrary system commands (e.g., rm -rf, DROP DATABASE, SHUTDOWN).
        The response MUST be a valid JSON list of strings. Example: ["<script>alert(1)</script>", "<img src=x onerror=alert(1)>"]
        """
        user_prompt = f"""
        Generate the safe testing payloads based on the following context:
        - Target Technologies: {context.get('technologies', ['unknown'])}
        - Injection Point Type: {context.get('injection_point_type', 'unknown')}
        - Target URL: {context.get('url', 'unknown')}
        - Target Parameter: {context.get('parameter', 'unknown')}
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        try:
            response_str = await self._call_llm(messages, is_json=True)
            raw_payloads = json.loads(response_str)

            if isinstance(raw_payloads, list) and all(isinstance(p, str) for p in raw_payloads):
                # Sanitize the payloads generated by the LLM before returning them
                sanitized_payloads = self._sanitize_and_filter_payloads(raw_payloads)
                return sanitized_payloads
            
            logger.warning("LLM returned a valid JSON but not a list of strings for payloads.")
            return []
        except (json.JSONDecodeError, APIError) as e:
            logger.error(f"Failed to generate or parse payloads from Groq LLM: {e}")
            return []

    async def analyze_vulnerability(self, request: Dict[str, Any], response: Dict[str, Any], vuln_type: str) -> bool:
        """Use the Groq LLM to determine if a response indicates a vulnerability."""
        system_prompt = f"""
        You are a security analyst. Analyze the provided HTTP request and response to determine if it indicates a '{vuln_type}' vulnerability.
        Your answer MUST be a single word: "true" or "false".
        """
        user_prompt = f"""
        Analyze the following data:
        - Request Payload: {request.get('payload')}
        - Target: {request.get('target')}
        - Response Status Code: {response.get('status_code')}
        - Response Body (first 2000 chars): {response.get('body', '')[:2000]}

        Does this strongly indicate a likely '{vuln_type}' vulnerability?
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        try:
            decision = await self._call_llm(messages, max_tokens=5)
            return decision.strip().lower() == "true"
        except APIError as e:
            logger.error(f"Failed to analyze vulnerability with Groq LLM: {e}")
            return False

    async def classify_vulnerability(self, target: Dict[str, Any], payload: str, analysis: Dict[str, Any], vuln_type: str) -> Dict[str, Any]:
        """Use the Groq LLM to classify and score a vulnerability finding."""
        system_prompt = f"""
        You are a vulnerability assessment expert. A potential '{vuln_type}' vulnerability was found.
        Based on the data, generate a vulnerability report as a single, valid JSON object.
        The JSON object MUST include these keys: 'type', 'severity' (one of 'critical', 'high', 'medium', 'low'),
        'title', 'description', 'remediation', and 'confidence' (a float between 0.0 and 1.0).
        """
        user_prompt = f"""
        Generate the JSON report for a '{vuln_type}' vulnerability based on this data:
        - Target: {json.dumps(target)}
        - Payload: "{payload}"
        - Analysis Indicators: {json.dumps(analysis.get('indicators'))}
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        try:
            report_str = await self._call_llm(messages, max_tokens=700, is_json=True)
            vuln_data = json.loads(report_str)

            # Add non-LLM generated data
            vuln_data['location'] = {
                "url": target.get("url", "unknown"),
                "parameter": target.get("injection_point", {}).get("name", "unknown"),
                "injection_type": target.get("injection_point", {}).get("type", "unknown"),
                "method": target.get("method", "GET")
            }
            vuln_data['evidence'] = {
                "payload": payload,
                "indicators": analysis.get('indicators'),
                "response_excerpt": ""
            }
            return vuln_data
        except (json.JSONDecodeError, APIError) as e:
            logger.error(f"Failed to classify vulnerability with Groq LLM: {e}")
            return {}